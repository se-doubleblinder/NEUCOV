{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a10467",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/AlaFalaki/AttentionVisualizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6ebaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('NeuralCC') \n",
    "from run_classifier import get_coverage, convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b7a0cb-67f3-4f2f-8272-ff4f647e1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import AttentionVisualizer as av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb890795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import argparse\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "\n",
    "\n",
    "class CodeCoverageMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CodeCoverageMLP, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(1 * config.hidden_size, config.hidden_size)\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(config.hidden_size, 1)\n",
    "        self.forward_activation = torch.nn.GELU()\n",
    "        # Dropout layer\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None \\\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # Add first hidden layer\n",
    "        x = self.forward_activation(self.fc1(x))\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # Add second hidden layer\n",
    "        x = self.forward_activation(self.fc2(x))\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # Add output layer\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        outputs = torch.sigmoid(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class CodeCoveragePredictionModel(nn.Module):\n",
    "    def __init__(self, args, config, tokenizer):\n",
    "        super(CodeCoveragePredictionModel, self).__init__()\n",
    "        self.max_tokens = args.max_tokens\n",
    "        self.use_statement_ids = args.use_statement_ids\n",
    "        self.roberta = RobertaModel.from_pretrained(args.model_key, config=config)\n",
    "        self.roberta.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        if self.use_statement_ids:\n",
    "            self.statement_embeddings = nn.Embedding(args.max_tokens, config.hidden_size)\n",
    "            print(self.statement_embeddings)\n",
    "\n",
    "        for param in self.roberta.parameters():\n",
    "            if args.pretrain:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.coverage_mlp = CodeCoverageMLP(config)\n",
    "        self.loss_criterion = nn.BCELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, inputs_ids, inputs_masks, statements_ids, test_input_statements_ids=None, gold_ids=None):\n",
    "        try:\n",
    "            coverage_labels=gold_ids\n",
    "            device = inputs_ids.device if inputs_ids is not None else 'cpu'\n",
    "            inputs_embeddings = self.roberta.embeddings.word_embeddings(inputs_ids)\n",
    "            if self.use_statement_ids: \n",
    "                statements_ids[statements_ids == -999] = 511    \n",
    "                inputs_embeddings += self.statement_embeddings(statements_ids)\n",
    "            \n",
    "            roberta_outputs = self.roberta(\n",
    "                inputs_embeds=inputs_embeddings,\n",
    "                attention_mask=inputs_masks,\n",
    "                output_attentions = True,\n",
    "                output_hidden_states = True,\n",
    "            )\n",
    "            \n",
    "            # Extract hidden states and attentions from roberta_outputs\n",
    "            hidden_states = roberta_outputs[2]  # Assuming hidden states are at index 2\n",
    "            hidden_states = roberta_outputs.hidden_states\n",
    "            # attentions = roberta_outputs[3]\n",
    "            attention_scores = roberta_outputs.attentions\n",
    "            outputs_embeddings = hidden_states[-1]\n",
    "            \n",
    "            batch_preds, batch_true = [], []\n",
    "            batch_loss = torch.tensor(0, dtype=torch.float, device=device)\n",
    "    \n",
    "            for _id, item_output_embeddings in enumerate(outputs_embeddings):\n",
    "                statements_embeddings = []\n",
    "                \n",
    "                ### Padding tokens in statements_ids is -999.\n",
    "                if self.use_statement_ids: \n",
    "                    item_statements_ids =  statements_ids[_id][\n",
    "                        torch.ne(statements_ids[_id], 511)\n",
    "                    ].tolist()\n",
    "                else:\n",
    "                    item_statements_ids =  statements_ids[_id][\n",
    "                        torch.ne(statements_ids[_id], -999)\n",
    "                    ].tolist()\n",
    "                # Statement Embeddings\n",
    "                # Tensor of item_statements_ids\n",
    "                item_statements_ids_tensor = torch.tensor(item_statements_ids, device=device)\n",
    "                \n",
    "                # Get the total length of the code\n",
    "                num_statements_in_item = torch.max(item_statements_ids_tensor).item()\n",
    "                # print(f\"Number of Lines in the Code: {num_statements_in_item}\")\n",
    "\n",
    "                # statement Embeddings\n",
    "                for sid in range(num_statements_in_item + 1):\n",
    "                    _statement_ids = (item_statements_ids_tensor == sid).nonzero().squeeze()\n",
    "                    statement_embedding = torch.mean(item_output_embeddings[_statement_ids], dim=0)\n",
    "                    statements_embeddings.append(statement_embedding)\n",
    "                               \n",
    "                # item_preds = self.coverage_mlp(torch.stack([torch.cat((x, test_input_embedding)) for x in statements_embeddings])).squeeze()\n",
    "                item_preds = self.coverage_mlp(torch.stack(statements_embeddings)).squeeze()\n",
    "                batch_preds.append(item_preds)\n",
    "\n",
    "                return batch_preds, attention_scores\n",
    "        except Exception as e:\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_dir = './dataset/testing.json'\n",
    "args.output_dir = './output/'\n",
    "args.max_tokens = 512\n",
    "args.save_predictions = True\n",
    "args.train_batch_size = 32\n",
    "args.eval_batch_size = 32\n",
    "args.learning_rate = 1e-4\n",
    "args.weight_decay = 0.0\n",
    "args.adam_epsilon = 1e-8\n",
    "args.max_source_size = 512\n",
    "args.use_statement_ids = False\n",
    "args.model_key = \"microsoft/codeexecutor\"\n",
    "args.pretrain = True\n",
    "args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72dc6e18-73e4-4a9e-bc5e-d8f0c245b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from AttentionVisualizer.main import *\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def indent_dedent_tokenize(code, tokenizer):\n",
    "    # Indent and Dedent\n",
    "    current_num_white_spaces = 0\n",
    "    prev_num_white_spaces = 0\n",
    "    # Token List\n",
    "    code_tokens_list = []\n",
    "    # Classification\n",
    "    num_tokens = 0\n",
    "    token_string = \"\"\n",
    "    sentence_id = []\n",
    "    \n",
    "    lines = code.split('\\n')\n",
    "    for line_number, line in enumerate(lines):\n",
    "        indented_code = \"\"\n",
    "        # line_number_token = f\"<{line_number}>\"\n",
    "        # indented_code += line_number_token + \" \"\n",
    "        current_num_white_spaces = len(re.match(r'^\\s*', line).group(0))\n",
    "        \n",
    "        if current_num_white_spaces > prev_num_white_spaces:\n",
    "            indented_code += \"<indent> \"\n",
    "            prev_num_white_spaces = current_num_white_spaces\n",
    "        elif current_num_white_spaces < prev_num_white_spaces:\n",
    "            diff = prev_num_white_spaces - current_num_white_spaces\n",
    "            for temp in range(diff // 4):\n",
    "                indented_code += \"<dedent> \"\n",
    "            prev_num_white_spaces = current_num_white_spaces\n",
    "            \n",
    "        # Remove Blank Space before each line \n",
    "        line = line.lstrip()\n",
    "        indented_code += line + \"\\n\"     \n",
    "        # Tokenize each line\n",
    "        code_tokens_of_one_line = tokenizer.tokenize(indented_code)\n",
    "        num_tokens += len(code_tokens_of_one_line)\n",
    "        # Sentence Id\n",
    "        sentence_id.extend([line_number] * len(code_tokens_of_one_line))\n",
    "        # Add particular line token into the string:\n",
    "        for token in code_tokens_of_one_line:\n",
    "            token_string += f'\"{str(token)}\" '\n",
    "            # Add Each token to the main list:\n",
    "            code_tokens_list.append(token)\n",
    "    return code_tokens_list\n",
    "\n",
    "def convert_examples_to_features(code, tokenizer):\n",
    "    code_tokens = indent_dedent_tokenize(code, tokenizer)\n",
    "    max_source_size = 512\n",
    "    # Encoder-Decoder for Trace Generation\n",
    "    source_tokens = code_tokens[ : max_source_size ] \n",
    "    source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "\n",
    "    source_masks = [1 for _ in range(len(source_ids))]\n",
    "    zero_padding_length = max_source_size - len(source_ids)\n",
    "\n",
    "    source_ids += [tokenizer.pad_token_id for _ in range(zero_padding_length)]\n",
    "    source_masks += [tokenizer.pad_token_id for _ in range(zero_padding_length)]\n",
    "\n",
    "    return torch.unsqueeze(torch.tensor(source_ids), dim=0), torch.unsqueeze(torch.tensor(source_masks), dim=0)\n",
    "\n",
    "\n",
    "class CustomAttentionVisualizer(av.AttentionVisualizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_name = \"microsoft/codeexecutor\"\n",
    "        self.tokenizer  = RobertaTokenizer.from_pretrained(self.model_name)   \n",
    "        special_tokens_list = ['<line>', '<state>', '</state>', '<dictsep>', '<output>', '<indent>',\n",
    "                                '<dedent>', '<mask0>']\n",
    "        for i in range(200):\n",
    "            special_tokens_list.append(f\"<{i}>\")\n",
    "        special_tokens_dict = {'additional_special_tokens': special_tokens_list}\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(\"microsoft/codeexecutor\")\n",
    "        self.model = CodeCoveragePredictionModel(args, config, self.tokenizer)\n",
    "\n",
    "        self.model.load_state_dict(torch.load('NeuralCC/model/model.ckpt', map_location=torch.device('cpu')), strict=False)\n",
    "        self.model = self.model.roberta\n",
    "\n",
    "        self.step1_lbl = widgets.HTML(value = f\"<h3>Step 1:</h3> Write the input text in the box below.\")\n",
    "        self.input_text = widgets.Textarea(placeholder='Type something', description='Input Text:', rows=10)\n",
    "        self.step1 = widgets.VBox([self.step1_lbl, self.input_text])\n",
    "        \n",
    "        self.preview_config_lbl = widgets.HTML(value = f\"<h3>Step 2:</h3> Select which attention layer/head and words to visualize.\")\n",
    "        self.ignore_specials  = widgets.Checkbox(value=True, description='Ignore BOS/EOS', indent=False)\n",
    "        self.ignore_dots     = widgets.Checkbox(value=True, description='Ignore [dot]s', indent=False)\n",
    "        self.ignore_stopwords = widgets.Checkbox(value=True, description='Ignore Stop Words', indent=False)\n",
    "        self.options = widgets.HBox([self.ignore_specials, self.ignore_dots, self.ignore_stopwords])\n",
    "        \n",
    "        self.layer_range = widgets.IntRangeSlider(value=[3, 5], min=1, max=12, step=1,\n",
    "                                continuous_update=False, orientation='horizontal', readout=True,\n",
    "                                readout_format='d')\n",
    "        self.layer_ind = widgets.Dropdown(options=range(1, 13), value=1)\n",
    "        self.layer = widgets.Dropdown(options=['all', 'range', 'individual'], value='all', description='Layer')\n",
    "        self.layer_selection = widgets.HBox([self.layer, self.layer_range, self.layer_ind])\n",
    "\n",
    "        self.head_range = widgets.IntRangeSlider(value=[3, 5], min=1, max=12, step=1,\n",
    "                                continuous_update=False, orientation='horizontal', readout=True,\n",
    "                                readout_format='d')\n",
    "        self.head_ind = widgets.Dropdown(options=range(1, 13), value=1)\n",
    "        self.head = widgets.Dropdown(options=['all', 'range', 'individual'], value='all', description='Head')\n",
    "        self.head_selection = widgets.HBox([self.head, self.head_range, self.head_ind])\n",
    "        \n",
    "        self.visualize_btn = widgets.Button(description='VISUALIZE')\n",
    "        self.note_lbl = widgets.HTML(value = f\"<small>Hold your cursor on each word for a second to see its attention score.</small>\")\n",
    "\n",
    "        self.out = widgets.HTML(layout={'border': '1px solid black', 'padding': '4px', 'margin-top': '10px'})\n",
    "        \n",
    "        self.step2 = widgets.VBox([self.preview_config_lbl, self.options,\n",
    "                                   self.layer_selection, self.head_selection, self.visualize_btn,\n",
    "                                   self.note_lbl, self.out])\n",
    "        \n",
    "        self.ui = widgets.VBox([self.step1, self.step2])\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # Get the input from textarea, do the preprocessing and run it through the model.\n",
    "    #--------------------------------------------------------------------------------\n",
    "    def on_visualize_click(self, c):\n",
    "        self.out.value = \"\" \n",
    "        input1, input2 = convert_examples_to_features(self.input_text.value, self.tokenizer)\n",
    "        outputs = self.model(input1, input2, output_attentions=True)\n",
    "        the_tokens = indent_dedent_tokenize(self.input_text.value, self.tokenizer)\n",
    "        number_of_tokens = len(the_tokens)\n",
    "        positions, dot_positions, stopwords_positions = find_positions(self.ignore_specials.value,\n",
    "                                                                       self.ignore_stopwords.value,\n",
    "                                                                       the_tokens,\n",
    "                                                                       self.stop_words)\n",
    "        the_words = make_the_words(the_tokens, positions, self.ignore_specials.value)\n",
    "\n",
    "        layer_indexes, head_indexes = self.extract_indexes()\n",
    "        the_scores = []\n",
    "        for i in range(*layer_indexes):\n",
    "            for ii in range(*head_indexes):\n",
    "                the_scores.append( torch.sum(outputs.attentions[i][0][ii], dim=0) / number_of_tokens )\n",
    "        \n",
    "        the_scores = torch.stack( the_scores )\n",
    "        final_score = torch.sum( the_scores, dim=0 ) / the_scores.size(0)\n",
    "\n",
    "        # Remove the CLS/SEP tokens and dots\n",
    "        if self.ignore_specials.value:\n",
    "            final_score = final_score[1:-1]\n",
    "            \n",
    "        min_ = torch.min( final_score )\n",
    "        \n",
    "        if self.ignore_dots.value:\n",
    "            final_score[list(dot_positions.values())] = min_\n",
    "\n",
    "        if self.ignore_stopwords.value:\n",
    "            final_score[list(stopwords_positions.values())] = min_\n",
    "        max_ = torch.max( final_score )\n",
    "        \n",
    "        for i in range( final_score.size(0) ):\n",
    "            final_score[i] = scale( final_score[i], min_, max_ )\n",
    "        self.final_score = final_score\n",
    "        \n",
    "        the_html = make_html(the_words, positions, final_score)\n",
    "        self.out.value = the_html\n",
    "        \n",
    "        del input1, outputs, the_scores, final_score\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def find_positions(ignore_specials, ignore_stopwords, the_tokens, stop_words):\n",
    "    dot_positions = {}\n",
    "    stopwords_positions = {}\n",
    "    tmp = []\n",
    "\n",
    "    if ignore_specials:\n",
    "        word_counter = 0\n",
    "        start_pointer = 0\n",
    "        positions = {}\n",
    "\n",
    "        num_of_tokens = len( the_tokens )\n",
    "        num_of_tokens_range = range( num_of_tokens + 1 )\n",
    "\n",
    "    else:\n",
    "        word_counter = 1\n",
    "        start_pointer = 1\n",
    "        positions = {0: [0, 1]}\n",
    "\n",
    "        num_of_tokens = len( the_tokens ) - 1\n",
    "        num_of_tokens_range = range( 1, num_of_tokens + 1 )\n",
    "\n",
    "\n",
    "    for i in num_of_tokens_range:\n",
    "        if i == num_of_tokens:\n",
    "            positions[word_counter] = [start_pointer, i]\n",
    "            break\n",
    "\n",
    "        if the_tokens[i][0] in ['Ġ', '.'] or \"Ċ\" == the_tokens[i]:\n",
    "            if ignore_stopwords:\n",
    "                joined_tmp = \"\".join(tmp)\n",
    "                current_word = joined_tmp[1:] if joined_tmp[0] == \"Ġ\" else joined_tmp\n",
    "                if current_word in stop_words:\n",
    "                    stopwords_positions[word_counter] = i-1\n",
    "\n",
    "            if the_tokens[i] == \".\":\n",
    "                dot_positions[word_counter+1] = i\n",
    "\n",
    "            positions[word_counter] = [start_pointer, i]\n",
    "            word_counter += 1\n",
    "            start_pointer = i\n",
    "            tmp = []\n",
    "\n",
    "        tmp.append(the_tokens[i])\n",
    "\n",
    "    if not ignore_specials:\n",
    "        positions[len( positions )] = [i, i+1]\n",
    "    \n",
    "    return positions, dot_positions, stopwords_positions\n",
    "\n",
    "\n",
    "def make_html(the_words, positions, final_score):\n",
    "    the_html = \"\\t\\t\"\n",
    "    line_ctr = 2\n",
    "    for i, word in enumerate( the_words ):\n",
    "        new_line = False\n",
    "        if \"Ċ\" in word:\n",
    "            new_line = True\n",
    "        word = word.replace(\"Ċ\", \"\")\n",
    "        word = word.replace(\"Ġ\", \"\")\n",
    "        \n",
    "        if i in positions:\n",
    "            start = positions[i][0]\n",
    "            end   = positions[i][1]\n",
    "\n",
    "            if end - start > 1:\n",
    "                score = torch.max( final_score[start:end] )\n",
    "            else:\n",
    "                score = final_score[start]\n",
    "\n",
    "            if new_line:\n",
    "                if line_ctr in [5, 7]:\n",
    "                    the_html += f\"<br />&emsp;&emsp;\"\n",
    "                else:\n",
    "                    the_html += f\"<br />\"\n",
    "                line_ctr += 1\n",
    "\n",
    "            \n",
    "            the_html += \"\"\"<span style=\"background-color:rgba(51, 153, 255, {});\n",
    "                        padding:3px 6px 3px 6px; margin: 0px 2px 0px 2px\" title=\"{}\"><code style=\"color:black\">{}</code></span>\"\"\".format(score, score, word)\n",
    "    \n",
    "    return the_html\n",
    "\n",
    "def make_the_words(the_tokens, positions, ignore_specials):\n",
    "    the_words = []\n",
    "    for [start, end] in positions.values():\n",
    "        the_words.append(''.join(the_tokens[start:end]))\n",
    "\n",
    "    return the_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''def main():\n",
    "    n = 2\n",
    "    if n % 2 == 1:\n",
    "        print(n // 2 + 1)\n",
    "    else:\n",
    "        print(n // 2)\n",
    "if __name__ == \"__main__\":\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5a0a1-ed99-4def-8226-fd6ca465305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    obj = CustomAttentionVisualizer()\n",
    "    obj.show_controllers(with_sample=True)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2915752-6767-4568-a4b3-4c437624dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct, num_incorrect = [], []\n",
    "for ex in examples_correct:\n",
    "    num_correct.append(len([line for line in ex.code.split('\\n') if 'aliasingVar' in line]))\n",
    "\n",
    "for ex in examples_incorrect:\n",
    "    num_incorrect.append(len([line for line in ex.code.split('\\n') if 'aliasingVar' in line]))\n",
    "\n",
    "import statistics\n",
    "print(f\"Correct\\tMean:{statistics.mean(num_correct)}\\tMedian: {statistics.median(num_correct)}\")\n",
    "print(f\"Incorrect\\tMean:{statistics.mean(num_incorrect)}\\tMedian: {statistics.median(num_incorrect)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
